{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://python.langchain.com/docs/tutorials/retrievers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run into an `import` problem in vscode, make sure you select the right python interpreter `> Python: Select Interpreter` and kernel `Notebook: Select Notebook Kernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import dotenv\n",
    "from httpx import ConnectError\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.operations import SearchIndexModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Project path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/firoozas/Documents/AI-from-scratch/RAG\n"
     ]
    }
   ],
   "source": [
    "current_dir = Path(os.getcwd())\n",
    "\n",
    "# # method 1: based on the root dir name\n",
    "# root_dir_name = 'RAG'\n",
    "# for p in current_dir.parents:\n",
    "# if p.name.lower() == root_dir_name.lower():\n",
    "#     root_dir = p\n",
    "#     break\n",
    "# else:\n",
    "#     raise Exception(f\"Root dir \\\"{root_dir_name}\\\" Not found\")\n",
    "\n",
    "# method 2: based on the \".git\" dir presence\n",
    "for p in current_dir.parents:\n",
    "    if \".git\" in os.listdir(current_dir.parent) or \".project-root\" in os.listdir(current_dir.parent):\n",
    "        root_dir = current_dir.parent\n",
    "        print(root_dir)\n",
    "        break\n",
    "else:\n",
    "    raise Exception(\"No root directory was found that contains a .git directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load variables into env\n",
    "f = root_dir / \".secrets\" / \".env\"\n",
    "assert f.exists(), f\"File not found: {f}\"\n",
    "dotenv.load_dotenv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Orange;'>LangChain Document Object</span>\n",
    "\n",
    "LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n",
    "\n",
    "- `page_content`: a string representing the content;\n",
    "- `metadata`: a dict containing arbitrary metadata;\n",
    "- `id`: (optional) a string identifier for the document.\n",
    "\n",
    "The metadata attribute can capture **information about the source** of the document, its **relationship to other documents,** and other information. \n",
    "\n",
    "> _Note that an individual Document object often represents a chunk of a larger document._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\", metadata={\"source\": \"mammal-pets-doc\"}\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:Khaki;'>Document loaders</span>\n",
    "\n",
    "DocumentLoaders load data into the standard LangChain Document format.\n",
    "\n",
    "https://python.langchain.com/docs/integrations/document_loaders/\n",
    "\n",
    "\n",
    "See this guide for more detail on PDF document loaders.\n",
    "\n",
    "https://python.langchain.com/docs/how_to/document_loader_pdf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(docs)=200\n",
      "CONTENT\n",
      "described in Chapter 4). If VMD is unable to guess the appropriate \fle type or guesses incorrectly,\n",
      "you must select it from the list manually.\n",
      "You can control into which VMD molecule you want to load \n",
      "\n",
      "{'source': '/Users/firoozas/Documents/AI-from-scratch/RAG/data/vmd_sample.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "file_path = root_dir / \"data\" / \"vmd_sample.pdf\"\n",
    "assert os.path.exists(file_path)\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"{len(docs)=}\")\n",
    "print(\"CONTENT\")\n",
    "print(f\"{docs[37].page_content[:200]}\\n\")\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:LightGreen;'>Splitting Text</span>\n",
    "\n",
    "Further splitting the PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\n",
    "\n",
    "We will split our documents **into chunks of 1000 characters with 200 characters of overlap** between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the [`RecursiveCharacterTextSplitter`](https://python.langchain.com/docs/how_to/recursive_text_splitter/), which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `add_start_index=True`` will preserve the character index where each split Document starts within the initial Document, as a metadata attribute “start_index”.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Orange;'>Embeddings</span>\n",
    "\n",
    "Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text.\n",
    "\n",
    "\n",
    "### <span style='color:Khaki;'>Installing Ollama</span>\n",
    "\n",
    "- [Installing Ollama](https://github.com/ollama/ollama?tab=readme-ov-file#ollama)\n",
    "\n",
    "- [Available models](https://ollama.com/search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OllamaEmbeddings(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both embedding have same length: True\n",
      "Generated vectors of length 3072\n",
      "\n",
      "[0.0018432532, -0.025140692, 0.014092629, -0.023098888, -0.0020301666]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # embedding example\n",
    "    vector_1 = embeddings_model.embed_query(all_splits[0].page_content)\n",
    "    vector_2 = embeddings_model.embed_query(all_splits[1].page_content)\n",
    "\n",
    "    embedding_length = len(vector_1)\n",
    "\n",
    "    print(\"Both embedding have same length:\", len(vector_1) == len(vector_2))\n",
    "    print(f\"Generated vectors of length {embedding_length}\\n\")\n",
    "    print(vector_1[:5])\n",
    "except ConnectError as e:\n",
    "    print(e)\n",
    "    print(\"Please install and run Ollama server locally or use the hosted version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:Orange;'>Vector stores</span>\n",
    "\n",
    "##### <span style='color:Khaki;'>Option 1: Local Qdrant Vector DB</span>\n",
    "\n",
    "```bash\n",
    "pip install qdrant-client llama-index\n",
    "```\n",
    "\n",
    "```python\n",
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "\n",
    "# Specify the collection (DB) name\n",
    "collection_name = \"chat_with_docs\"\n",
    "\n",
    "# Initialize Qdrant client\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")\n",
    "\n",
    "# Configure the Qdrant vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "# Set up the storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Create the Vector Store Index\n",
    "index = VectorStoreIndex(\n",
    "    nodes,  # This should be a list of document nodes using (llama_index.Document.from_text())\n",
    "    storage_context=storage_context\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "### <span style='color:Khaki;'>Option 2: Cloud MongoDB Atlas Vector DB</span>\n",
    "[Available DBs](https://python.langchain.com/docs/integrations/vectorstores/)\n",
    "\n",
    "```bash\n",
    "pip install \"pymongo[srv]\"\n",
    "```\n",
    "\n",
    "###### <span style='color:LightGreen;'>Notes</span>\n",
    "\n",
    "1. The `relevance_score_fn` parameter (in `MongoDBAtlasVectorSearch()`) in the client library ensures that the client understands how the relevance scores returned by MongoDB should be interpreted. It will not automatically create a \"vector search index\". It should be created manually.\n",
    "   - Cosine Similarity: Returns values typically between -1 (opposite) and 1 (identical).\n",
    "   - Euclidean Distance: Returns non-negative values where 0 means identical, and larger numbers indicate greater dissimilarity.\n",
    "   - Dot Product: Returns unbounded values where higher scores indicate greater similarity.\n",
    "\n",
    "2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ping: Successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "# https://www.mongodb.com/docs/manual/reference/connection-string/\n",
    "# https://swethag04.medium.com/rag-using-mongodb-atlas-vector-search-and-langchain-cba57b67fe29\n",
    "# https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/\n",
    "\n",
    "\n",
    "class AtlasClient:\n",
    "    def __init__(self, atlas_uri=None, dbname: str = None, collection_name: str = None, index_name: str = None):\n",
    "        if atlas_uri is None:\n",
    "            atlas_uri = os.getenv(\"MONGODB_URI\")\n",
    "        if atlas_uri is None:\n",
    "            raise ValueError(\"Please provide a valid MongoDB Atlas URI or set MONGODB_URI in .env file\")\n",
    "\n",
    "        self._clt = MongoClient(atlas_uri)\n",
    "        self.database = self._clt[dbname] if dbname is not None else None\n",
    "        self.collection = self.database[collection_name] if self.database is not None else None\n",
    "        self._init_vector_store = False\n",
    "        self.index_name = index_name\n",
    "\n",
    "    # A quick way to test if we can connect to Atlas instance\n",
    "    def ping(self, debug=True):\n",
    "        try:\n",
    "            self._clt.admin.command(\"ping\")\n",
    "            print(\"Ping: Successfully connected to MongoDB!\") if debug else None\n",
    "        except Exception as e:\n",
    "            print(\"Ping:\", e)\n",
    "            print(\n",
    "                \"You may need to add your IP address to Network Access list in MongoDB deployment\\n\"\n",
    "                \"https://cloud.mongodb.com -> Security -> Network Access\"\n",
    "            ) if debug else None\n",
    "\n",
    "    def create_indexes(self, embedding_model, index_name: str, index_def: dict = None):\n",
    "        existing_indexes = [d[\"name\"] for d in list(client.collection.list_search_indexes())]\n",
    "        if index_name in existing_indexes:\n",
    "            print(f\"Index {index_name} already exists.\")\n",
    "            return\n",
    "        \n",
    "        # get the embedding length\n",
    "        embedding_length = len(embedding_model.embed_query(\"test\"))\n",
    "        # Define the vector search index\n",
    "        vector_search_index_definition = (\n",
    "            {\"fields\": [{\"type\": \"vector\", \"path\": \"embedding\", \"similarity\": \"cosine\", \"numDimensions\": embedding_length}]}\n",
    "            if index_def is None\n",
    "            else index_def\n",
    "        )\n",
    "        # Create the search index model\n",
    "        search_index_model = SearchIndexModel(definition=vector_search_index_definition, name=index_name, type=\"vectorSearch\")\n",
    "        # Create the index on the collection\n",
    "        self.collection.create_search_index(model=search_index_model)\n",
    "        self.index_name = index_name\n",
    "        self._similarity = vector_search_index_definition[\"fields\"][0][\"similarity\"]\n",
    "\n",
    "    def init_vector_store(self, embedding_model, score_fn: str = None):\n",
    "        if self.collection is None or self.index_name is None:\n",
    "            raise ValueError(\"Run reinit(...) with db, collection, and index names as needed.\")\n",
    "\n",
    "        if score_fn is None and self._similarity is not None:\n",
    "            score_fn = self._similarity\n",
    "        elif score_fn is None:\n",
    "            score_fn = \"cosine\"\n",
    "\n",
    "        print(f\"Using similarity function: {score_fn} and index: {self.index_name}\")\n",
    "\n",
    "        self._vector_store = MongoDBAtlasVectorSearch(\n",
    "            embedding=embedding_model, collection=self.collection, index_name=self.index_name, relevance_score_fn=score_fn\n",
    "        )\n",
    "        self._init_vector_store = True\n",
    "\n",
    "    @property\n",
    "    def vector_store(self):\n",
    "        if not self._init_vector_store:\n",
    "            raise ValueError(\"Please run init_vector_store(...) first.\")\n",
    "        return self._vector_store\n",
    "\n",
    "    # init a new collection\n",
    "    def reinit(self, dbname: str = None, collection_name: str = None, index_name: str = None):\n",
    "        self.database = self._clt[dbname] if dbname is not None else self.database\n",
    "        self.collection = self.database[collection_name] if collection_name is not None else self.collection\n",
    "        self.index_name = index_name if index_name is not None else self.index_name\n",
    "\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = AtlasClient(dbname=\"VMD_RAG\", collection_name=\"VMD_PDF\")\n",
    "client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index pdf_cosine already exists.\n"
     ]
    }
   ],
   "source": [
    "client.create_indexes(embeddings_model, index_name=\"pdf_cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having instantiated our vector store, we can now index the documents.\n",
    "if len(all_splits) != client.collection.count_documents({}):\n",
    "    client.init_vector_store(embeddings_model)\n",
    "    ids = client.vector_store.add_documents(documents=all_splits)\n",
    "else:\n",
    "    print(\"Documents already indexed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
